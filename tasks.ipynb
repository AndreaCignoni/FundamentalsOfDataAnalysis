{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Data Analysis Tasks\n",
    "\n",
    "**Andrea Cignoni**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "> The Collatz conjecture is a famous unsolved problem in mathematics. The problem is to prove that if you start with any positive integer $x$ and repeatedly apply the function $f(x)$ below, you always get stuck in the repeating sequence 1, 4, 2, 1, 4, 2, . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines the calculation applied to the input number 'x'\n",
    "def f(x):\n",
    "    if x % 2 == 0: #This \"if\" clause checks whether the integer given is even or not\n",
    "        return x // 2 #If it is even it is devided by two\n",
    "    else:\n",
    "        return (3 * x) + 1 #If it is odd it then multiplied by 3 and then 1 is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function formats the sequence of integers generated with the 'x' function\n",
    "def collatz (x):\n",
    "    while x != 1: #This 'while' clause stops the loop at number 1\n",
    "        print(x, end=', ')\n",
    "        x = f(x) # Update 'x' with the format defined in the collatz function\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collatz(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "> Give an overview of the famous penguins data set,2 explaining the types of variables it contains. Suggest the types of variables that should be used to model them in Python, explaining your rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# **PENGUINS DATA SET**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "The Palmer penguins dataset is a collection of data about penguins in the Palmer Archipelago, Antarctica. These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network and were collected on 344 penguins living on three islands (Torgersen, Biscoe, and Dream).<br/>\n",
    "\n",
    "The parameters considered are the followings:   \n",
    "\n",
    "\n",
    "+ Island name (Dream, Torgersen, or Biscoe);\n",
    "- Species name (Adelie, Chinstrap, or Gentoo);\n",
    "* Billl length (mm);\n",
    "+ Bill depth (mm);\n",
    "- Flipper length (mm);\n",
    "* Body mass (g);\n",
    "- Sex.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "![Screenshot](Penguins.png) ![Screenshot](Penguins1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "**LOADING THE FILE AND LIBRARIES IMPORTED**<br/>\n",
    "\n",
    "The famous Palmer penguins dataset is downloaded from mwaskom/seaborn-data ![Github](https://github.com/mwaskom/). Since the information appears readable and structured just as the standard CSV tabular disposition (a comma separates individual items and each record is on a new line), I have proceeded to open it as such in my Python repository.\n",
    "\n",
    "Before proceeding to load the data in a data frame, I have proceeded to import the necessary modules to analyse all the variable represented in the file: Pandas; Matplotlib; Numpy; Seaborn.<br/>\n",
    "\n",
    "- Pandas\n",
    "\n",
    "Pandas is an open source Python package that is used for data science/data analysis and machine learning tasks. The common operations performed with Pandas are also: data cleansing, data fill, data normalization, merges and joins, data visualization, statistical analysis, data inspection, loading and saving data and much more. Here, I rely on Pandas for indexing the data frame, for manipulating it and extracting the sorted information from specified columns and rows. My main source for its usage is pandas.pydata.org\n",
    "\n",
    "- Matplotlib\n",
    "\n",
    "Matplotlib is a Python library used to create 2D graphs and plots through scripts. The pyplot module is explecially useful when it comes to control line styles, font properties, formatting axes etc. It supports a very wide variety of graphs and plots namely - histogram, bar charts, power spectra, error charts etc.\n",
    "\n",
    "- Numpy\n",
    "\n",
    "Matplotlib is used along with NumPy to provide an environment with an effective and fast numeric computing. Numpy furnishes a multidimensional array object and various derived objects (such as masked arrays and matrices).\n",
    "\n",
    "- Seaborn\n",
    "\n",
    "Lastly, in order to clearly display a graphic overview of the whole dataset through pair plots, I have imported Seaborn. This library is built on top of the Matplotlib data visualization library and can perform the exploratory analysis that fits best to show the result of my searches.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reading and formatting the data set downloaded from (https://github.com/mwaskom/seaborn-data/blob/master/penguins.csv)\n",
    "\n",
    "data = pd.read_csv('penguins.csv', header=None)\n",
    "\n",
    "# Extracting penguin dataset's 8 variables for representation\n",
    "\n",
    "file=open(\"penguins.csv\", \"a\")\n",
    "\n",
    "data.columns= ['species','island','bill_lenght_mm','bill_depth_mm','flipper_lenght_mm','body_mass_g','sex']\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "**DATA DESCRIPTION**\n",
    "\n",
    "In order to give a visual overview of the data contained in the dataset, I have utilised a number of functions provided by Pandas referring to realpython.com. As already pointed out, the observations of the penguins are classified through 8 classes of prameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\",\"Below are shown the first and the last five rows of the dataset\")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the DataFrame structure\n",
    "print(\"\\n\",\"This is the summary of the dataframe, including information about the column names, data types, and non-null values:\",\"\\n\")\n",
    "\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to display stats about data\n",
    "print(\"\\n\",\"These are the main statistical information of the dataset:\",\"\\n\")\n",
    "\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observation taken for each species\n",
    "print(\"\\n\",\"Number of samples for each class:\",\"\\n\")\n",
    "\n",
    "print(data['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "**PREPROCESSING THE DATA SET**\n",
    "\n",
    "\n",
    "Before proceeding to my actual analysis, I have preprocessed the data contained in the penguins dataset. This standard procedure is used to remove missing or inconsistent data values resulting from human or computer error. Preprocessing data can significantly improve the accuracy and quality of a dataset, making it more reliable. Once the data are proved to be consistent and all unhelpful parts are eliminated, the information can be transformed into a format that is more easily and effectively processed in data mining, machine learning and other data science tasks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset: check for null values\n",
    "print(\"\\n\",\"Below, the missing values that were found in the raw file:\",\"\\n\")\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
